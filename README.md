# ðŸš€ Roadmap: QuasarML

This roadmap outlines the development plan for building a **feature-rich scientific computing & ML library** in C++ with Vulkan acceleration.  
The goal is to combine the usability of **NumPy** with the deep learning power of **PyTorch**.

---

## âœ… Phase 1 â€” Core Foundations (MVP)
**Goal:** A minimal but usable Vulkan-accelerated tensor library.  

 - [x] Tensor class (shape, strides, dtype, device, ownership flags)  
 - [x] Memory management (CPU + GPU buffers, host-device transfers)  
 - [x] Basic elementwise ops (add, mul, sub, div)  
 - [x] pow  
 - [x] Reductions: sum (axis)  
 - [x] mean  
 - [x] min  
 - [x] max  
 - [x] Broadcasting rules (NumPy-style)  
 - [x] Indexing/slicing

### Recent progress (summary)

- Implemented GPU-first non-contiguous slicing using a generic strided-gather compute shader (no CPU fallback).
- Added Nâ€‘D two-pass reductions (sum/mean/min/max) and pow; all Phase 1 functionality now implemented and covered by unit tests.
- Unit tests: full suite passes locally (15/15) after fixes to view ownership and descriptor handling.

---

## âœ… Phase 2 â€” NumPy Parity
**Goal:** Become a NumPy alternative in C++.  

- [ ] More elementwise ops (exp, log, sin, cos, sqrt, etc.)  
- [ ] Advanced indexing (boolean masks, fancy indexing)  
 - [x] Matrix operations: matmul, transpose, reshape, flatten  
 - [ ] dot (alias / convenience)  
- [ ] Random number generation (uniform, normal, Bernoulli)  
- [ ] I/O basics (save/load tensors from disk, NumPy `.npy` support)  

---

## âœ… Phase 3 â€” Autograd Engine
**Goal:** Add PyTorch-style automatic differentiation.  

- [ ] Computation graph (track ops + parents)  
- [ ] Backpropagation (chain rule)  
- [ ] `.grad` fields on tensors  
- [ ] Support for in-place ops  
- [ ] Custom gradient support  

---

## âœ… Phase 4 â€” Neural Network API
**Goal:** Build the `torch.nn` equivalent.  

- [ ] Modules / Layers (Linear, Conv2D, Dropout, BatchNorm)  
- [ ] Activations (ReLU, Sigmoid, Tanh, GELU)  
- [ ] Loss functions (MSE, CrossEntropy)  
- [ ] Optimizers (SGD, Adam)  
- [ ] Parameter management (save/load weights)  

---

## âœ… Phase 5 â€” Performance & Vulkan Magic
**Goal:** Make it fast and scalable.  

 - [x] Vulkan kernel library (optimized shaders for ops)  
- [ ] Kernel fusion (combine multiple ops into one dispatch)  
- [ ] Mixed precision support (FP16, BF16)  
- [ ] Memory pool / caching (avoid malloc/free overhead)  
- [ ] Multi-GPU support (basic)  

---

## âœ… Phase 6 â€” ML Ecosystem Features
**Goal:** Add developer convenience features.  

- [ ] Data loaders (batching, shuffling)  
- [ ] Metrics (accuracy, precision, F1)  
- [ ] Callbacks (checkpoints, LR schedulers)  
- [ ] Logging/profiling utilities  

---

## âœ… Phase 7 â€” Advanced & Research-Level Features
**Goal:** Compete with cutting-edge ML frameworks.  

- [ ] Transformers (Attention, MHA, LayerNorm, etc.)  
- [ ] Distributed training (multi-node, NCCL-like over Vulkan)  
- [ ] Sparse tensor support  
- [ ] Quantization / pruning for inference  
- [ ] ONNX model import/export  

---

## ðŸ”‘ Key Advice
- **Phases 1â€“2 (Core + NumPy parity)** are the foundation.  
- **Phase 3 (autograd)** makes it PyTorch-like.  
- **Phases 4â€“5 (nn + performance)** make it competitive.  
- **Phases 6â€“7** are advanced and can come later.  